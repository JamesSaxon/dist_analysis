{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from fiona.crs import from_epsg\n",
    "\n",
    "from glob import glob\n",
    "\n",
    "import re, json\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_context(\"notebook\", font_scale = 1.5)\n",
    "sns.set_style(\"white\", rc={\"figure.figsize\": (2.5, 0.5), \"axes.linewidth\" : 2})\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "method_full = {\"axis_ratio\" : \"Axis Ratio\", \"dist_a\" : \"Areal Distance\", \"dist_p\" : \"Population Distance\",\n",
    "               \"dyn_radius\" : \"Dynamic Radius\", \"ehrenburg\" : \"Inscribed Circles\", \"exchange\" : \"Exchange\", \n",
    "               \"harm_radius\" : \"Harmonic Radius\", \"hull_a\" : \"Hull Area\", \"hull_p\" : \"Hull Population\", \n",
    "               \"inertia_a\" : \"Inertia Area\", \"inertia_p\" : \"Inertia Population\", \"mean_radius\" : \"Mean Radius\", \n",
    "               \"polsby\" : \"Isoperimeter Quotient\", \"polsby_w\" : \"County-Weighted IPQ\",\n",
    "               \"power\" : \"Power Diagram\", \"reock\" : \"Circumscribing Circles\", \n",
    "               \"rohrbach\" : \"Distance to Perimeter\", \"split\" : \"Split-Line\", \"path_frac\" : \"Path Fraction\",\n",
    "               \"107\" : \"107th Congress\", \"111\" : \"111th Congress\", \"114\" : \"114th Congress\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring split counties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To replicate or not to replicate?\n",
    "Change this flag to pull data from my private database instead of the prepared files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "REPLICATION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here, I am caching for replication the tracts' intersecting \"membership\" in counties and _enacted_ Congressional Districts.  At the block level (block group for 1990), this is fairly \"hefty\": the database tables are several GB, and they have only point geometries.  So I am saving after the double group by statement (i.e., by county AND cd)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cache_cd_sessn_split(usps, sessn):\n",
    "    \n",
    "    table = \"census_blocks_2010\"\n",
    "    geom  = \"geom\"\n",
    "    if int(sessn) == 111:\n",
    "        table = \"census_blocks_2000\"\n",
    "    if int(sessn) == 107:\n",
    "        table = \"census_bg_1990\"\n",
    "        geom  = \"centroid\"\n",
    "\n",
    "    # Join points to CDs using within.\n",
    "    # Cunties are already part of the GEOID hierarchy.\n",
    "    cd = pd.read_sql(\"SELECT county, cd, SUM(pop) pop \"\n",
    "                     \"FROM {} b \"\n",
    "                     \"JOIN cd ON \"\n",
    "                     \"  b.state = cd.state AND \"\n",
    "                     \"  ST_Within(b.{}, cd.geom) \"\n",
    "                     \"JOIN states s ON \"\n",
    "                     \"  b.state = s.fips \"\n",
    "                     \"WHERE pop > 0 AND usps = UPPER('{}') AND sessn = {} \"\n",
    "                     \"GROUP BY county, cd \"\n",
    "                     \"ORDER BY county, cd;\".format(table, geom, usps, sessn), \n",
    "                     con = cen_con)\n",
    "    \n",
    "    # GROUP and SUM by county and cd.\n",
    "\n",
    "    cd.to_csv(\"data/cd_splits/{}_{}.csv\".format(usps, sessn), index = False)\n",
    "\n",
    "\n",
    "if not REPLICATION:\n",
    "    \n",
    "    for usps in [\"md\", \"nc\", \"pa\"]:\n",
    "        print(usps, \"::\", end = \" \")\n",
    "        for sessn in [107, 111, 114]:\n",
    "            print(sessn, end = \" \")\n",
    "            cache_cd_sessn_split(usps, sessn)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the split fraction as defined in the paper: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cd_share(usps, sessn):\n",
    "\n",
    "    cd = pd.read_csv(\"data/cd_splits/{}_{}.csv\".format(usps, sessn))\n",
    "\n",
    "    cd_sum        = cd.groupby(\"cd\")            [\"pop\"].sum().reset_index().rename(columns = {\"pop\" : \"cd_pop\"})\n",
    "    county_sum    = cd.groupby(\"county\")        [\"pop\"].sum().reset_index().rename(columns = {\"pop\" : \"county_pop\"})\n",
    "    cd_county_sum = cd.groupby([\"cd\", \"county\"])[\"pop\"].sum().reset_index().rename(columns = {\"pop\" : \"cd_county_pop\"})\n",
    "\n",
    "    splits = cd_county_sum.merge(cd_sum).merge(county_sum)\n",
    "    splits[\"Xpop\"] = splits[[\"county_pop\", \"cd_pop\"]].min(axis = 1)\n",
    "    splits[\"frac\"] = splits[\"cd_county_pop\"] / splits[\"Xpop\"]\n",
    "\n",
    "    s = (splits.frac * splits[\"cd_county_pop\"]).sum() / splits[\"cd_county_pop\"].sum()\n",
    "\n",
    "    return [s]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the regionalizations for MD, PA, and NC to three compressed files.  \n",
    "\n",
    "This comes of not anticipating this question by referees -- so the \"replication\" is a bit awkward.\n",
    "\n",
    "The processed files are written to `data/splits/??_regions.csv.bz2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "jdir = \"data/c4_redux/\"\n",
    "s3_files = \"/media/jsaxon/brobdingnag/data/s3/res/{}/final.csv\"\n",
    "\n",
    "polsby_w_json  = \"/home/jsaxon/proj/cluscious/res/json/{}_polsby_w*.json\"\n",
    "polsby_w_files = \"/home/jsaxon/proj/cluscious/res/{}/final.csv\"\n",
    "\n",
    "def get_state_files(s):\n",
    "\n",
    "    files = {}\n",
    "    \n",
    "    print(s, end = \" :: \")\n",
    "    \n",
    "    # Go through the json redux and load the corresponding csvs\n",
    "    #   if it satisfies the population check.\n",
    "    with open(jdir + \"/{}_redux.json\".format(s.lower())) as fi:\n",
    "        for line in fi:\n",
    "\n",
    "            j = json.loads(line)\n",
    "\n",
    "            m = j[\"UID\"].split(\"/\")[1]\n",
    "\n",
    "            if m == \"axis_ratio\":\n",
    "                if j[\"PopulationDeviation\"] > 0.05: continue\n",
    "            elif j[\"PopulationDeviation\"] > 0.02: continue\n",
    "\n",
    "            if m not in files: files[m] = []\n",
    "\n",
    "            files[m].append(s3_files.format(j[\"UID\"]))\n",
    "\n",
    "            \n",
    "    # Now do the same thing for the weighted polsby files.\n",
    "    files[\"polsby_w\"] = []\n",
    "    for line in list(glob(polsby_w_json.format(s))):\n",
    "        \n",
    "        with open(line) as json_file:\n",
    "            j = json.load(json_file)\n",
    "            if j[\"PopulationDeviation\"] > 0.02: continue\n",
    "            files[\"polsby_w\"].append(polsby_w_files.format(j[\"UID\"]))\n",
    "            \n",
    "    # Save all these as as single file.\n",
    "    # THIS is what comes of not anticipating the regionalization.\n",
    "    regionalization = []\n",
    "\n",
    "    for m in files:\n",
    "        print(m, end = \" \")\n",
    "\n",
    "        for f in files[m]:\n",
    "            df = pd.read_csv(f, header = None, names = [\"index\", \"cd\"])\n",
    "\n",
    "            df[\"method\"] = m\n",
    "\n",
    "            fmeta = f.split(\"/\")\n",
    "            df[\"uid\"] = fmeta[-3] + \"/\" + fmeta[-2]\n",
    "\n",
    "            regionalization.append(df)\n",
    "\n",
    "    # Assemble the regionalizations from the individual CSVs/DFs.\n",
    "    # Sort, reorder, and compress.\n",
    "    regionalization = pd.concat(regionalization, sort = False).reset_index(drop = True)\n",
    "    regionalization.sort_values([\"method\", \"uid\"], inplace = True)\n",
    "    regionalization = regionalization[[\"method\", \"uid\", \"index\", \"cd\"]]\n",
    "    regionalization.to_csv(\"data/splits/{}_regions.csv.bz2\".format(s), \n",
    "                           compression = \"bz2\", index = False)\n",
    "                        \n",
    "    print()\n",
    "    return files\n",
    "\n",
    "\n",
    "if not REPLICATION:\n",
    "    \n",
    "    get_state_files(\"md\")\n",
    "    get_state_files(\"nc\")\n",
    "    get_state_files(\"pa\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate the level of county splits for a dataframe.\n",
    "\n",
    "This will be run as an \"apply\" function on a grouped dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tract_county_pop = pd.read_csv(\"data/tract_race.csv\", usecols = [\"usps\", \"rn\", \"cid\", \"pop\"])\n",
    "tract_county_pop.set_index(\"usps\", inplace = True)\n",
    "tract_county_pop.rename(columns = {\"cid\" : \"county\", \"rn\" : \"index\"}, inplace = True)\n",
    "\n",
    "def split_rate(data, usps):\n",
    "    \n",
    "    # Get merge it with the county label and population.\n",
    "    data = data.copy().merge(tract_county_pop.loc[usps.upper()])[[\"index\", \"cd\", \"county\", \"pop\"]]\n",
    "\n",
    "    # Group by county and CD, and then together,\n",
    "    # to get the populations of the CD, county, and intersection.\n",
    "    cd_sum        = data.groupby(\"cd\")            [\"pop\"].sum().reset_index().rename(columns = {\"pop\" : \"cd_pop\"})\n",
    "    county_sum    = data.groupby(\"county\")        [\"pop\"].sum().reset_index().rename(columns = {\"pop\" : \"county_pop\"})\n",
    "    cd_county_sum = data.groupby([\"cd\", \"county\"])[\"pop\"].sum().reset_index().rename(columns = {\"pop\" : \"cd_county_pop\"})\n",
    "\n",
    "    # The split fraction is the intersection pop. divided by \n",
    "    #   the lesser of the CD and the county pop.\n",
    "    splits = cd_county_sum.merge(cd_sum).merge(county_sum)\n",
    "    splits[\"Xpop\"] = splits[[\"county_pop\", \"cd_pop\"]].min(axis = 1)\n",
    "    splits[\"frac\"] = splits[\"cd_county_pop\"] / splits[\"Xpop\"]\n",
    "\n",
    "    # This is the mean value, weighted by each fragment of the population.\n",
    "    # That is, each individual intersection of a county and a Congressional district \n",
    "    # has a split fraction and a population.  The split fraction average\n",
    "    # is weighted by the population, for the state-wide average.\n",
    "    sval = (splits.frac * splits[\"cd_county_pop\"]).sum() / splits[\"cd_county_pop\"].sum()\n",
    "    \n",
    "    # print(splits)\n",
    "\n",
    "    return sval\n",
    "\n",
    "    shares = pd.Series([sval])\n",
    "    shares.columns = [\"split_frac\"]\n",
    "    return shares\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potting Function\n",
    "\n",
    "The plotting function here is nearly identical to the ones used in `main_plots.ipynb`.  The only real difference is the bounds, which are specified by the user.  Like competitiveness, this is a continuous variable..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "size = (2.2, 0.5)\n",
    "bins = np.arange(0, 1.001, 0.01)\n",
    "def plot_common_county(data, state, method, vmin, vmax):\n",
    "\n",
    "    # sns.set(rc={\"figure.figsize\": size, \"axes.linewidth\" : 4})\n",
    "    f, ax = plt.subplots(1, sharex=True, sharey=True, figsize = size)\n",
    "\n",
    "\n",
    "    if len(data) > 1:\n",
    "\n",
    "        ## Warning about normed -> density keyword, \n",
    "        ##   from matplotlib through seaborn.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            sns.distplot(data, ax = ax, bins = bins,\n",
    "                         norm_hist = True, kde = False, \n",
    "                         hist_kws={\"alpha\" : 1.0, \"color\" : \"#4DAFFF\"})\n",
    "\n",
    "    if len(data):\n",
    "\n",
    "        mval = sum(data) / len(data)\n",
    "\n",
    "        ## Warning about normed -> density keyword, \n",
    "        ##   from matplotlib through seaborn.\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "\n",
    "            ax.plot([mval, mval], ax.get_ylim(), \n",
    "                    linewidth = 3, linestyle = \"solid\", \n",
    "                    c = \"r\" if method[0] == \"1\" else \"k\")\n",
    "\n",
    "\n",
    "    sns.despine(left = True)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"\")\n",
    "    ax.set_yticks([])\n",
    "    \n",
    "    ax.set_xticks([0.6, 0.7, 0.8])\n",
    "    ax.set_xlim(vmin, vmax)\n",
    "\n",
    "    f.savefig(\"splits/{}_{}_ax.pdf\".format(state, method), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    ax.set_xticks([])\n",
    "    f.savefig(\"splits/{}_{}.pdf\".format(state, method), bbox_inches='tight', pad_inches=0)\n",
    "\n",
    "    plt.close(\"all\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "md :: axis_ratio dist_a dist_p dyn_radius ehrenburg exchange harm_radius hull_a hull_p inertia_a inertia_p mean_radius path_frac polsby polsby_w power reock rohrbach split 107 111 114 \n",
      "nc :: axis_ratio dist_a dist_p dyn_radius ehrenburg exchange harm_radius hull_a hull_p inertia_a inertia_p mean_radius path_frac polsby polsby_w power reock rohrbach split 107 111 114 \n",
      "pa :"
     ]
    }
   ],
   "source": [
    "vmin = {\"md\" : 0.53, \"nc\" : 0.65, \"pa\" : 0.55}\n",
    "vmax = {\"md\" : 0.79, \"nc\" : 0.88, \"pa\" : 0.88}\n",
    "\n",
    "communities_df = []\n",
    "for s in [\"md\", \"nc\", \"pa\"]:\n",
    "    \n",
    "    print(s, end = \"\")\n",
    "    communities = []\n",
    "    # community_data = {}\n",
    "\n",
    "    # Read in all of the data (!)\n",
    "    state_regions = pd.read_csv(\"data/splits/{}_regions.csv.bz2\".format(s))\n",
    "    # state_regions.set_index(\"method\", inplace = True)\n",
    "    \n",
    "    # Now groupby method and uid to get the average split value for each map.\n",
    "    print(\" :\", end = \"\")\n",
    "    sim_shares = state_regions.groupby([\"method\", \"uid\"]).apply(split_rate, usps = s)\n",
    "    del state_regions\n",
    "    print(\": \", end = \"\")\n",
    "    \n",
    "    # Simulated plans\n",
    "    for m in sorted(method_full):\n",
    "    \n",
    "        if \"1\" in m: continue\n",
    "        print(m, end = \" \")\n",
    "            \n",
    "        communities.append([m, sim_shares.loc[m].mean(),\n",
    "                            \"XZfigs/splits/{}_{}ZX\".format(s.lower(), m)])\n",
    "        \n",
    "        # community_data[m] = list(shares)\n",
    "        \n",
    "    # for m, shares in community_data.items():\n",
    "        plot_common_county(list(sim_shares.loc[m]), s, m, vmin[s], vmax[s])\n",
    "\n",
    "    # Enacted plans\n",
    "    for m in [\"107\", \"111\", \"114\"]:\n",
    "    \n",
    "        print(m, end = \" \")\n",
    "        shares = cd_share(s, m)\n",
    "        communities.append([m, sum(shares) / len(shares),\n",
    "                            \"XZfigs/splits/{}_{}ZX\".format(s.lower(), m)])\n",
    "        plot_common_county(shares, s, m, vmin[s], vmax[s])\n",
    "                \n",
    "\n",
    "    print()\n",
    "\n",
    "    cdf = pd.DataFrame(communities, columns = [\"Method\", \"Mean\", \"Figure\"])\n",
    "    cdf.replace({\"Method\" : method_full}, inplace = True)\n",
    "    cdf.set_index([\"Method\"], inplace = True)\n",
    "        \n",
    "    communities_df.append(cdf)\n",
    "    \n",
    "pd.options.display.precision = 2\n",
    "cdf = pd.concat(communities_df, axis = 1)\n",
    "cdf.columns = pd.MultiIndex.from_product([[\"Maryland\", \"North Carolina\", \"Pennsylvania\"],\n",
    "                                          [\"Mean\", \"Figure\"]])\n",
    "\n",
    "cdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index_order = ['107th Congress', '111th Congress', '114th Congress', \n",
    "               'Areal Distance', 'Axis Ratio', 'Circumscribing Circles', \n",
    "               'Distance to Perimeter', 'Dynamic Radius', 'Exchange', \n",
    "               'Harmonic Radius', 'Hull Area', 'Hull Population', \n",
    "               'Inertia Area', 'Inertia Population', 'Inscribed Circles', \n",
    "               'Isoperimeter Quotient', 'County-Weighted IPQ', \n",
    "               'Mean Radius', 'Path Fraction', 'Population Distance', \n",
    "               'Power Diagram', 'Split-Line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# cdf.sort_index(inplace = True)\n",
    "cdf = cdf.loc[index_order]\n",
    "tex = cdf.to_latex(na_rep = \"\", column_format = \"l\" + \"c\" * 6,\n",
    "                                multicolumn_format = \"c\")\n",
    "\n",
    "caption = \"\"\"\n",
    "Probability of a citizen of a county living in the same congressional district\n",
    "  as another randomly-selected citizen of their county, for distributions of maps\n",
    "  derived for various compactness definitions.\n",
    "The ``probability\" is modified to account for counties larger than\n",
    "  Congressional Districts (see text).\n",
    "North Carolina gained a seat after the 2000 Census, \n",
    "  whereas Pennsylvania lost seats in both 2002 and 2012.\n",
    "The probabilities do not not correct for this effect (first three rows).\n",
    "There is a progression towards more-dividied counties\n",
    "  among the enacted maps.\n",
    "For Maryland and North Carolina, the automated procedures \n",
    "  outperform the enacted maps.\n",
    "The county-weighted isoperimeter quotient measure outperforms \n",
    "  baseline IPQ by a few percent.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "tex = re.sub(r\" *(Maryland) \\& *\", r\" \\\\multicolumn{2}{c}{ \\\\selectfont \\1} \", tex)\n",
    "tex = re.sub(r\" *(North Carolina) \\& *\", r\" \\\\multicolumn{2}{c}{ \\\\selectfont \\1} \", tex)\n",
    "tex = re.sub(r\" *(Pennsylvania) \\& *\", r\" \\\\multicolumn{2}{c}{ \\\\selectfont \\1} \", tex)\n",
    "\n",
    "tex = re.sub(r\".*Mean.*\\n\", \"\", tex)\n",
    "tex = re.sub(r\".*Method.*\\n\", \"\", tex)\n",
    "\n",
    "tex = tex.replace(\"splitZX\", \"split_axZX\")\n",
    "\n",
    "tex = tex.replace(\"XZ\", \"\\includegraphics[width=6.5em]{\")\n",
    "tex = tex.replace(\"\\_\", \"_\")\n",
    "tex = tex.replace(\"ZX\", \"}\")\n",
    "\n",
    "# tex = tex.replace(\"nan\", \"\")\n",
    "tex = re.sub(\"None\", \"\\\\\\includegraphics[width=6.5em]{mini_hist/blank}\", tex)\n",
    "\n",
    "tex = tex.replace(\"toprule\", \"hline \\\\hline \\\\\\\\ \")\n",
    "tex = tex.replace(\"\\midrule\", \"\\\\\\\\ \\\\hline \\\\\\\\\")\n",
    "tex = tex.replace(\"\\\\bottomrule\", \"\\hline \\hline\")\n",
    "\n",
    "tex = tex + \"\\caption{\" + caption + \"}\"\n",
    "tex = tex + \"\\label{tab:competitiveness}\"\n",
    "\n",
    "tex = \"\\n\\\\begin{table}\\n\\\\renewcommand{\\\\arraystretch}{0.7}\\n \" + tex + \"\\n\\\\end{table}\\n \"\n",
    "\n",
    "tex = tex.replace(\"figs/\", \"\")\n",
    "\n",
    "tex = tex.replace(\"Areal Distance\", \"\\\\\\\\ \\hline \\\\\\\\ \\nAreal Distance\")\n",
    "\n",
    "tex = re.sub(r\"(Split-Line)(.*)(0.[0-9]{2})(.*)(0.[0-9]{2})(.*)(0.[0-9]{2})\", \n",
    "             r\"\\\\raisebox{1.3em}{\\1} \\2 \\\\raisebox{1.3em}{\\3} \\4 \\\\raisebox{1.3em}{\\5} \\6 \\\\raisebox{1.3em}{\\7} \", tex)\n",
    "\n",
    "# tex = \"^NT:\\n\\n\" + tex\n",
    "\n",
    "with open(\"tex/splits_table.tex\", \"w\") as o: o.write(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
